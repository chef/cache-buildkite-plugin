#!/bin/bash
set -ue

DIR="$(cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd)"

# shellcheck disable=SC1090
. "$DIR/../lib/shared.bash"

paths=( $(plugin_read_list CACHED_FOLDERS) )

for path in "${paths[@]}"
do
  # if the path is bad - skip it, don't break things
  if [[ -d ${path} ]]; then
    filename=`echo ${path} |sed 's/\//_/g'`

    # remove trailing '-' from filename
    if [[ ${filename: -1} == "_" ]]; then
      filename=${filename::${#filename}-1}
    fi

    echo "compressing ${path}"
    if [[ $OSTYPE != "msys" ]] ; then
      sudo tar -czf ${filename}.tar.gz ${path}
    else
      tar -czf ${filename}.tar.gz ${path}
    fi

    # remove '/' as it breaks S3 pathing
    # remove ' ' as it breaks aws s3 cp command
    label=${BUILDKITE_LABEL//\//}
    label=${label// /_}

    s3_bucket="s3://${BUILDKITE_PLUGIN_CACHE_S3_BUCKET}/${BUILDKITE_PIPELINE_SLUG}/${label}"

    aws s3 ls ${s3_bucket}/${filename}.tar.gz
    rc=$?
    if [[ ${rc} -eq 0 ]]; then
      md5s3object=$(aws s3api head-object --bucket ${BUILDKITE_PLUGIN_CACHE_S3_BUCKET} --key ${BUILDKITE_PIPELINE_SLUG}/${label}/${filename}.tar.gz | jq -r '.Metadata.md5checksum')
      md5localobject=$(md5 ${filename}.tar.gz |base64)

      if [[ ${md5s3object} == ${md5localobject} ]]; then
        echo "skipping upload, checksums are identical"
      else
        echo "copying cache into s3"
        aws s3 cp ${filename}.tar.gz ${s3_bucket}/${filename}.tar.gz --metadata md5checksum=${md5localobject}
      fi
    else
      echo "copying cache into s3"
      aws s3 cp ${filename}.tar.gz ${s3_bucket}/${filename}.tar.gz --metadata md5checksum=${md5localobject}
    fi
  else
    echo "skipping, ${path} is not a directory"
  fi
done
